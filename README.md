[![author](https://img.shields.io/badge/author-brunobonatini-red.svg)](https://www.linkedin.com/in/bsbonatini)

# Projeto: Engenharia de dados com Docker, Python, Spark, Delta, SQL, LocalStack e AWS

Este projeto demonstra a constru√ß√£o de um pipeline completo de Engenharia de Dados, simulando um ambiente pr√≥ximo ao produtivo, com ingest√£o, processamento, valida√ß√£o, governan√ßa e disponibiliza√ß√£o anal√≠tica de dados utilizando tecnologias amplamente adotadas no mercado.

O pipeline foi desenvolvido com Apache Spark, Delta Lake e AWS (simulado via LocalStack), seguindo boas pr√°ticas de arquitetura de dados, como separa√ß√£o por camadas (Raw, Stage e Analytics), controle de qualidade de dados, versionamento de tabelas, processamento incremental e observabilidade por meio de logs e testes automatizados.

Os dados de entrada s√£o provenientes de um arquivo Excel contendo informa√ß√µes de clientes e endere√ßos, que passam por regras de valida√ß√£o, enriquecimento e transforma√ß√£o at√© serem disponibilizados para consumo anal√≠tico. Todo o ambiente √© containerizado com Docker, garantindo reprodutibilidade e facilidade de execu√ß√£o.

Al√©m do pipeline principal, o projeto inclui:

Cria√ß√£o program√°tica do Data Lake

Processamento incremental com Delta Lake (MERGE)

Testes de qualidade de dados com Pytest e Spark

Valida√ß√µes anal√≠ticas via Spark SQL e Jupyter Notebook

Estrutura preparada para integra√ß√£o com cat√°logo de dados e governan√ßa

Este projeto foi pensado como um case t√©cnico, evidenciando decis√µes arquiteturais, preocupa√ß√£o com qualidade, escalabilidade e organiza√ß√£o de c√≥digo, sendo ideal para portf√≥lio e avalia√ß√µes t√©cnicas em Engenharia de Dados.

# Como Executar o Projeto

Este guia descreve o passo a passo para executar o projeto localmente utilizando Docker, Spark, Delta Lake e LocalStack, sem necessidade de uma conta AWS real.

# 1. Pr√©-requisitos

Antes de iniciar, certifique-se de ter instalado:

	* Docker Desktop (vers√£o 20+)

	* Docker Compose (v2+)

	* Git (opcional, para clonar o reposit√≥rio)

‚ö†Ô∏è N√£o √© necess√°rio instalar Java, Spark ou Python localmente.
Todo o ambiente √© provisionado via Docker.

Manual de instala√ß√£o do Docker Desktop: https://docs.docker.com/desktop/setup/install/windows-install/

Verificar o WSL do Windows: https://docs.docker.com/desktop/setup/install/windows-install/#wsl-verification-and-setup


# 2. Estrutura do Projeto

<img width="808" height="534" alt="image" src="https://github.com/user-attachments/assets/2f1d4fc2-dbdd-4f5d-bf88-0371d290647e" />

# 3. Configura√ß√£o de Vari√°veis de Ambiente

Para execu√ß√£o local com LocalStack, crie o arquivo .env na pasta /scripts/projeto/:

<img width="461" height="274" alt="image" src="https://github.com/user-attachments/assets/7205a345-623d-4c3e-8e72-3fdaefaab2a3" />

Para execu√ß√£o em AWS real, basta ajustar as credenciais e remover o endpoint

# 4. Subir o Ambiente com Docker

O Docker ou Docker Desktop precisa estar em execu√ß√£o.

Na raiz do projeto (pasta seu-local/projeto-engenharia-de-dados-aws), abra um terminal ou prompt de comando e execute:

docker compose up -d --build

Esse comando ir√°:

	* Construir a imagem com Spark + Delta Lake

	* Subir o LocalStack (simulando o S3)

	* Iniciar o Jupyter Notebook

	* Montar o projeto dentro do container
	

# 5. Acessar o Container

Para acessar o terminal do container Spark:

docker exec -it projeto-aws-ed bash

Dentro do container, navegue at√© o a pasta /repositorio/projeto:

cd /repositorio/projeto


# 6. Criar a Estrutura do Data Lake (opcional)

Caso queira criar manualmente o bucket no LocalStack:

aws --endpoint-url=http://localstack:4566 s3 mb s3://data-lake-local

O pipeline cria essa estrutura automaticamente.


# 7. Executar o Pipeline Completo

Para rodar todo o fluxo (Ingest√£o ‚Üí Stage ‚Üí Analytics):

python3 pipeline.py

Esse processo executa, na ordem:

	* Ingest√£o Raw a partir do Excel

	* Escrita na camada Raw (Parquet + Snappy + Parti√ß√µes)

	* Processamento Stage com Delta Lake

	* Gera√ß√£o da camada Analytics
	
	* Gera√ß√£o de arquivos de logs


# 8. Validar os Dados no S3 (LocalStack)

Listar o conte√∫do do Data Lake:

aws --endpoint-url=http://localstack:4566 s3 ls s3://data-lake-local/

Exemplo de estrutura esperada:

raw/
stage/
analytics/


# 9. Valida√ß√£o via Jupyter Notebook

Acesse o Jupyter Notebook no navegador ou via Docker Desktop:

http://localhost:8888

Na pasta:

/projeto/notebooks/

Voc√™ encontrar√° notebooks para:

	* Valida√ß√£o da ingest√£o Raw

	* Valida√ß√£o da camada Stage

	* Valida√ß√£o da camada Analytics

	* Simula√ß√£o de cat√°logo com Spark SQL
	

# 10. Consultar Dados com Spark SQL

http://localhost:8888

Na pasta /projeto/src/athena cont√©m uma query anal√≠tica para Athena:

Esta query pode ser executada no Jupyter Notebook:

/projeto/notebooks/catalogo_spark_sql.ipynb


# 11. Executa√ß√£o dos Testes (adicional)

No terminal ou prompt de comando, na ra√≠z do projeto (pasta /repositorio/projeto/), execute os testes de valida√ß√£o:

pytest

Voc√™ deve ver os testes de qualidade passando com sucesso no arquivo de log gerado na pasta projeto/logs/tests.

tests.log

Data Quality tests

	* Log consolidado

	* Vis√£o do estado do dado

	* Ideal para auditoria e monitoramento

Voc√™ deve ver os testes de Unit passando com sucesso no arquivo de log gerado na pasta projeto/logs

validacao_clientes.log

validacao_enderecos.log

Unit tests

	* Log por m√≥dulo

	* Foco em regra de neg√≥cio

	* Arquivos pequenos e espec√≠ficos


# Resultados Esperados

Ao final das execu√ß√µes:

	* Dados estar√£o organizados em Raw, Stage e Analytics

	* Tabelas Delta criadas corretamente

	* Parti√ß√µes reconhecidas
	
	* Banco de dados anal√≠tico criado com Spark

	* Query para Athena executada com sucesso no banco de dados anal√≠tico

	* Testes realizados com sucesso

	* Pipeline totalmente reproduz√≠vel localmente


# 12. Encerrar o Ambiente

Sair do ambiente no Docker:

No terminal ou prompt de comando, em /repositorio/projeto execute:

exit

Para parar os containers:

docker compose down

Para remover as imagens criadas (opcional):

docker rmi projeto-aws-etl-spark

docker rmi localstack/localstack

======================================================================================================================	

# Decis√µes T√©cnicas

Esta se√ß√£o descreve as principais decis√µes t√©cnicas adotadas no projeto, bem como os motivos por tr√°s de cada escolha, considerando boas pr√°ticas de Engenharia de Dados, escalabilidade e clareza arquitetural.


1. Containeriza√ß√£o com Docker

O projeto foi containerizado para garantir:

	* Reprodutibilidade do ambiente

	* Padroniza√ß√£o das depend√™ncias

	* Facilidade de execu√ß√£o em qualquer m√°quina

O Dockerfile inclui:

	* Python

	* Pandas

	* Python-dotenv

	* Pytest

	* Openpyxl

	* Java + Spark

	* Conector S3A

	* Delta Lake

	* AWS CLI

	* Boto3

	* Jupyter Notebook para valida√ß√µes


2. Uso do Apache Spark como motor de processamento

O Apache Spark foi escolhido como engine principal por oferecer:

	* Processamento distribu√≠do e escal√°vel

	* API madura para transforma√ß√£o de dados estruturados

	* Integra√ß√£o nativa com formatos colunares (Parquet)

	* Suporte avan√ßado a camadas anal√≠ticas (SQL, DataFrames)

Mesmo tratando-se de um case com volume reduzido, a escolha do Spark simula um cen√°rio real de produ√ß√£o, onde o crescimento do volume de dados √© esperado.


3. Arquitetura em camadas (Raw, Stage e Analytics)

O Data Lake foi estruturado seguindo um padr√£o amplamente adotado no mercado:

	üîπ Raw

		* Armazena os dados brutos, sem altera√ß√µes sem√¢nticas

		* Mant√©m rastreabilidade total da origem dos dados

		* Dados particionados por data_processamento

	üîπ Stage

		* Aplica tipagem, deduplica√ß√£o e regras m√≠nimas de integridade

		* Utiliza Delta Lake para permitir cargas incrementais

		* Representa a camada de dados confi√°veis (clean data)

	üîπ Analytics

		* Camada orientada ao consumo anal√≠tico

		* Cont√©m regras de neg√≥cio (ex: clientes ativos, idade calculada)

		* Dados particionados por estado para otimizar consultas

Essa separa√ß√£o garante governan√ßa, auditabilidade e facilidade de manuten√ß√£o.


4. Ado√ß√£o do Delta Lake na camada Stage

O Delta Lake foi utilizado na camada Stage para resolver problemas comuns em Data Lakes tradicionais:

	* Falta de controle transacional

	* Dificuldade em cargas incrementais

	* Aus√™ncia de versionamento

Com Delta Lake foi poss√≠vel:

	* Realizar MERGE incremental por chave de neg√≥cio

	* Garantir consist√™ncia ACID

	* Permitir evolu√ß√£o futura do schema

	* Simular comportamento de tabelas de Data Warehouse


5. Valida√ß√£o de dados antes da ingest√£o

As regras de valida√ß√£o foram implementadas antes da escrita na camada Raw, utilizando Pandas, por tr√™s motivos principais:

	* Simplicidade para valida√ß√µes linha a linha

	* Clareza na gera√ß√£o de logs de rejei√ß√£o

	* Separa√ß√£o expl√≠cita entre dados v√°lidos e inv√°lidos

As valida√ß√µes incluem:

	* Campos obrigat√≥rios

	* Formato de CPF, e-mail e CEP

	* Integridade referencial entre clientes e endere√ßos

	* Valida√ß√£o de datas

Essa abordagem evita a propaga√ß√£o de dados inv√°lidos para camadas posteriores.


6. Testes automatizados para regras de valida√ß√£o

Foram criados testes unit√°rios com pytest para as fun√ß√µes de valida√ß√£o, garantindo que:

	* As regras de neg√≥cio sejam reproduz√≠veis

	* Altera√ß√µes futuras n√£o quebrem comportamentos esperados

	* O projeto tenha maior confiabilidade e manutenibilidade

	* A decis√£o de testar valida√ß√µes (e n√£o apenas transforma√ß√µes Spark) reflete pr√°ticas reais de projetos maduros.


7. Uso do LocalStack como alternativa ao AWS real

Para permitir desenvolvimento local sem depend√™ncia da AWS real, foi utilizado o LocalStack, simulando:

	* Amazon S3

	* Estrutura de Data Lake

	* Permiss√µes e endpoints

Essa escolha permitiu:

	* Execu√ß√£o do pipeline 100% local

	* Redu√ß√£o de custos

	* Facilidade de reprodu√ß√£o do ambiente

Limita√ß√µes do LocalStack, como a aus√™ncia do AWS Glue completo, foram tratadas com solu√ß√µes alternativas.


8. Simula√ß√£o de cat√°logo de dados com Spark SQL

Devido √† limita√ß√£o do LocalStack em emular completamente o AWS Glue, o cat√°logo de dados foi simulado usando Spark SQL:

	* Cria√ß√£o de tabelas externas via CREATE TABLE USING PARQUET

	* Gerenciamento de parti√ß√µes com MSCK REPAIR TABLE

	* Consultas anal√≠ticas diretamente via Spark SQL

Essa abordagem mant√©m o conceito de Data Catalog, mesmo fora da AWS real.


9. Uso do Parquet como formato de armazenamento

	* O formato Parquet foi escolhido por:

	* Armazenamento colunar

	* Compress√£o eficiente (Snappy)

	* Leitura seletiva de colunas

	* Ampla compatibilidade com ferramentas anal√≠ticas

Esse formato √© padr√£o de mercado para Data Lakes modernos.


10. Orquestra√ß√£o simples via Python com pipeline.py

O pipeline foi orquestrado por um script Python central (pipeline.py), respons√°vel por:

	* Criar a estrutura do Data Lake

	* Executar ingest√£o, stage e analytics em ordem

	* Centralizar a execu√ß√£o do fluxo completo

Essa abordagem mant√©m o projeto simples, por√©m facilmente evolutiva para ferramentas como Airflow ou Step Functions.


11. Uso do Pytest para Testes Automatizados

Para garantir a qualidade dos dados e a confiabilidade do pipeline, foi adotado o Pytest como framework de testes automatizados.

Motiva√ß√µes da escolha:

	* Framework padr√£o do ecossistema Python, amplamente utilizado na ind√∫stria

	* Sintaxe simples e expressiva, facilitando leitura e manuten√ß√£o dos testes

	* Excelente integra√ß√£o com Spark, DataFrames, fixtures e pipelines de dados

	* Suporte nativo a fixtures reutiliz√°veis, permitindo compartilhar a SparkSession entre testes

	* Facilidade de execu√ß√£o local, em containers Docker e em pipelines de CI/CD

Estrat√©gia de testes adotada:

	* Testes Unit√°rios (tests/unit)
	Respons√°veis por validar regras de neg√≥cio puras, sem depend√™ncia do Data Lake.

	* Testes de Qualidade de Dados (tests/data_quality)
	Respons√°veis por validar os dados reais persistidos no Data Lake, ap√≥s execu√ß√£o do pipeline.

Uso do conftest.py e Fixtures

Foi criado um arquivo conftest.py para centralizar:

	* Configura√ß√£o do PYTHONPATH

	* Cria√ß√£o e destrui√ß√£o da SparkSession

	* Compartilhamento eficiente da sess√£o entre testes


# Considera√ß√µes finais

As decis√µes t√©cnicas priorizaram:

	* Boas pr√°ticas de Engenharia de Dados

	* Clareza arquitetural

	* Simula√ß√£o de cen√°rios reais de produ√ß√£o

	* Facilidade de entendimento para fins de avalia√ß√£o t√©cnica

O projeto foi desenvolvido pensando em escalabilidade, governan√ßa e qualidade de dados, mesmo sendo um ambiente local e controlado.
